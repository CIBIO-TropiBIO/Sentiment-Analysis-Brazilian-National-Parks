{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47198489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88de79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 23 17:31:30 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 517.20       Driver Version: 517.20       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   53C    P0    24W /  N/A |      0MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7d0d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU Available'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "sns.set(rc={'figure.figsize':(10,6)})\n",
    "sns.set(font_scale=1.3)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import re\n",
    "import string\n",
    "import swifter\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification,Trainer, TrainingArguments, pipeline, AutoTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\"GPU Available\" if torch.cuda.is_available() else \"--Not available--\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe2522",
   "metadata": {},
   "source": [
    "# Loading training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec91c7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217364, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#B2W Digital, one of the most prominent Latin American e-commerce, released the B2W-Reviews01, \n",
    "#an open corpus of product reviews with more than 130,000 user reviews. This dataset has two target features: \n",
    "#the binary label \"recommend to a friend\", and a user rate from 1 to 5 stars. Here, we only considered the user rate.\n",
    "df_file = pd.read_csv(\"data2/archive/b2w.csv\", sep=',')\n",
    "\n",
    "#The Corpus Buscapé is a large corpus of Portuguese product reviews crawled in 2013 with more than 80,000 samples \n",
    "#from the Buscapé, a product and price search website.\n",
    "#Source:  https://www.kaggle.com/datasets/fredericods/ptbr-sentiment-analysis-datasets\n",
    "\n",
    "df_file = pd.concat([df_file[['review_text','rating']],pd.read_csv(\"data2/archive/buscape.csv\", sep=',')[['review_text','rating']]])\n",
    "df_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48d135b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.373539\n",
       "4    0.303284\n",
       "1    0.140364\n",
       "3    0.127339\n",
       "2    0.055474\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_file.rating.value_counts()/df_file.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c463bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_truncate(content, length=280, suffix='...'):\n",
    "    if len(content) <= length:\n",
    "        return content\n",
    "    else:\n",
    "        return ' '.join(content[:length+1].split(' ')[0:-1])\n",
    "\n",
    "\n",
    "def limpa_texto(data):\n",
    "    \n",
    "    tx = data.apply(lambda x: re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',str(x)))\n",
    "    tx = tx.swifter.apply(lambda x: re.sub('@[^\\s]+',' ',str(x))) # remover os @usuario\n",
    "    tx = tx.swifter.apply(lambda x: re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', str(x))) # remover as hashtag\n",
    "    #tx = tx.swifter.apply(lambda x: convert_emoticons(x))\n",
    "    tx = tx.swifter.apply(lambda x: re.sub(u'[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]', '',str(x)))\n",
    "    #tx = tx.swifter.apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "    #tx = tx.swifter.apply(lambda x: ' '.join([x for x in x.split() if x not in stop_words]))\n",
    "\n",
    "    tx = tx.swifter.apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n",
    "    tx = tx.swifter.apply(lambda x: re.sub(' +', ' ', str(x))) # remover espaços em brancos\n",
    "    tx = tx.swifter.apply(lambda x: x.strip())\n",
    "    tx = tx.swifter.apply(lambda x: x.lower())\n",
    "    tx = tx.swifter.apply(lambda x: smart_truncate(x)) #Truncate maximum twitter length 280 characters\n",
    "     \n",
    "    return tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cfb98ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299a72757fed4f9d9d5f4d19e8bf46eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/217364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9723f558c17a4309947fa9bab621f897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/217364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811d45c401a147fdb1525335ca165fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/217364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19b8dc84e2d4c5ba84097962fbe75d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/217364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e667fd58e6549d3887dbeef445dcc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/217364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16619f869e4644c9960b0c27afaecde6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/217364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc1a4d91ce34eb39149556faf7460e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/217364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c648ed288b46f68621a216e0a6e760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/217364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Preprocessing tweet text\n",
    "df_file.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_file['tweet_text_limpo'] = limpa_texto(df_file.review_text)\n",
    "\n",
    "# Transform rating to sentiment (1,2:negative(=0) 3:neutral(=2) 4,5:positive(=1))\n",
    "sentiment = {1:0,2:0,3:2,4:1,5:1}\n",
    "df_file['sentiment'] = df_file['rating'].map(sentiment).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd0c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, validation_ds = train_test_split(df_file[['tweet_text_limpo','sentiment']],test_size=0.2, \n",
    "                                           stratify = df_file['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cdeae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_ds)\n",
    "validation_ds = Dataset.from_pandas(validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad375f7",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b71436",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b0160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"tweet_text_limpo\"], padding=True, truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1dc5ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c4343bd033487db7087be9ebee4b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds_encoded = train_ds.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "856f4971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b08b13687a9436c82c2ea826ca3a1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_ds_encoded = validation_ds.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748bd86",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5313de46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = 3\n",
    "model_ckpt = \"neuralmind/bert-base-portuguese-cased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=num_labels)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b4ab5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a326106",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "logging_steps = len(train_ds_encoded) // batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "training_args = TrainingArguments(output_dir = model_name,\n",
    "                                    num_train_epochs = 2,\n",
    "                                    learning_rate = 2e-5,\n",
    "                                    per_device_train_batch_size = batch_size,\n",
    "                                    per_device_eval_batch_size = batch_size,\n",
    "                                    weight_decay=0.01,\n",
    "                                    evaluation_strategy = \"epoch\",\n",
    "                                    disable_tqdm = False,\n",
    "                                    logging_steps = logging_steps,\n",
    "                                    push_to_hub = False,\n",
    "                                    log_level = \"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a62b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_encoded = train_ds_encoded.rename_column('tweet_text_limpo', 'text')\n",
    "train_ds_encoded = train_ds_encoded.rename_column('sentiment', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27683eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ds_encoded = validation_ds_encoded.rename_column('tweet_text_limpo', 'text')\n",
    "validation_ds_encoded = validation_ds_encoded.rename_column('sentiment', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9aeb74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling class imbalance\n",
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                 classes = np.unique(train_ds_encoded['label']),\n",
    "                                                 y = train_ds_encoded['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05e6a967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10870' max='10870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10870/10870 1:44:58, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.593700</td>\n",
       "      <td>0.560088</td>\n",
       "      <td>0.813746</td>\n",
       "      <td>0.828213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.502500</td>\n",
       "      <td>0.560897</td>\n",
       "      <td>0.804729</td>\n",
       "      <td>0.822059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights.astype(np.float32)).to('cuda'))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(model = model, \n",
    "                        args = training_args,\n",
    "                        compute_metrics = compute_metrics,\n",
    "                        train_dataset = train_ds_encoded,\n",
    "                        eval_dataset = validation_ds_encoded,\n",
    "                        tokenizer = tokenizer)\n",
    "\n",
    "\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17f086",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8aaca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('./sentiment_transfer_learning_transformer_union_buscape/')\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./sentiment_transfer_learning_transformer_buscape/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5175c",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4ff68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sentiment_transfer_learning_transformer_buscape/\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained('./sentiment_transfer_learning_transformer_buscape/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2baf4",
   "metadata": {},
   "source": [
    "# Predicting over the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4262903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel('data2/Test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bcf78e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDs aleatorios</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51954</td>\n",
       "      <td>Just posted a photo @ Parque Municipal das Ara...</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4190</td>\n",
       "      <td>ncêndio em unidade de conservação na Amazônia ...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65306</td>\n",
       "      <td>Lixeiras antifauna são testadas no Parque Naci...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105536</td>\n",
       "      <td>Parque Nacional da Tijuca abriga maior preguiç...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57593</td>\n",
       "      <td>#betacaralhudosan Vídeo mostra incêndio na par...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDs aleatorios                                               text sentiment\n",
       "0           51954  Just posted a photo @ Parque Municipal das Ara...    neutro\n",
       "1            4190  ncêndio em unidade de conservação na Amazônia ...  negativo\n",
       "2           65306  Lixeiras antifauna são testadas no Parque Naci...  positivo\n",
       "3          105536  Parque Nacional da Tijuca abriga maior preguiç...  positivo\n",
       "4           57593  #betacaralhudosan Vídeo mostra incêndio na par...  negativo"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d802668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=loaded_model.model\n",
    "model = loaded_model.to('cpu')\n",
    "classifier = pipeline(\"text-classification\", model=loaded_model,tokenizer=tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c020b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(text):\n",
    "    return classifier(text, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39ba8318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3643abdb786746969d72667cf792693e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e0a5bd510148e7a7434c9a26139c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef846829b6414fff912b68469bff928b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dafc7a97704326b2d1a11b94c95e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb9208052d0404486e98c66b53b84fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9f54f9c616458d8f0b61f953ebf88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a5e4d0592b45adb340581c351aa56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f955b4d2a294ea793f5daafae0bb762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test['tweet_text_limpo'] = limpa_texto(df_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77a93c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TROPIBIO\\mambaforge\\envs\\env_sentiment\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_test[\"preds\"] = df_test[\"tweet_text_limpo\"].apply(lambda text: make_predictions(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1e34600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probas(preds,tipo):\n",
    "    if (tipo=='positive'):\n",
    "        prob = preds[0][1]['score']\n",
    "    elif (tipo=='negative'):\n",
    "        prob = preds[0][0]['score']\n",
    "    else:\n",
    "        prob = preds[0][2]['score']\n",
    "    return prob\n",
    "\n",
    "df_test[\"positivo\"] = df_test[\"preds\"].apply(lambda text: probas(text,'positive'))\n",
    "df_test[\"negativo\"] = df_test[\"preds\"].apply(lambda text: probas(text,'negative'))\n",
    "df_test[\"neutro\"] = df_test[\"preds\"].apply(lambda text: probas(text,'neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50503d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sentiment_pred'] = df_test[['positivo','negativo','neutro']].idxmax(axis=1)\n",
    "\n",
    "sentiment = {\n",
    "    0:\"negativo\",\n",
    "    1:\"positivo\",\n",
    "    2:\"neutro\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "189266c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negativo       0.83      0.53      0.65       436\n",
      "      neutro       0.23      0.01      0.03       795\n",
      "    positivo       0.44      0.95      0.60       769\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.50      0.50      0.43      2000\n",
      "weighted avg       0.44      0.49      0.38      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_test[['sentiment']], df_test[['sentiment_pred']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46622d2d",
   "metadata": {},
   "source": [
    "# Predicting final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data2/sentiment_analysis_PN.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba21e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing tweet text\n",
    "df['tweet_text_limpo'] = limpa_texto(df.text_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preds\"] = df[\"tweet_text_limpo\"].apply(lambda text: make_predictions(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"positive\"] = df[\"preds\"].apply(lambda text: probas(text,'positive'))\n",
    "df[\"negative\"] = df[\"preds\"].apply(lambda text: probas(text,'negative'))\n",
    "df[\"neutral\"] = df[\"preds\"].apply(lambda text: probas(text,'neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df[['positive','negative','neutral']].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2ebad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicciones feitas -- Borrar\n",
    "df = pd.read_excel('sentiment_analysis_predicted_modelo_buscape.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96484d87",
   "metadata": {},
   "source": [
    "# Negative topic analysis in the six principal national parks in Brazil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a90d0a",
   "metadata": {},
   "source": [
    "## Topic model common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96a2ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\TROPIBIO/.cache\\torch\\sentence_transformers\\neuralmind_bert-base-portuguese-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\TROPIBIO/.cache\\torch\\sentence_transformers\\neuralmind_bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Topic model\n",
    "from bertopic import BERTopic\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=15, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=list(stop_words), ngram_range=(1, 2))\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.2)\n",
    "sentence_model = SentenceTransformer(\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7924e",
   "metadata": {},
   "source": [
    "#### PARQUE NACIONAL DO IGUAÇU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "f5326b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nega_igua = pd.DataFrame(df.loc[(df.sentiment=='negative') & (df.UC_text=='PARQUE NACIONAL DO IGUAÇU'),\n",
    "                                   'tweet_text_limpo']).reset_index(drop=True)\n",
    "\n",
    "df_nega_igua['tweet_text_limpo'] = df_nega_igua['tweet_text_limpo'].astype('string') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "88b0bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate BERTopic\n",
    "embeddings = sentence_model.encode(df_nega_igua['tweet_text_limpo'], show_progress_bar=False)\n",
    "\n",
    "#We set the parameter \"min_cluster_size=10\" due to the greater number of observations for this park\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, \n",
    "                        metric='euclidean', \n",
    "                        prediction_data=True)\n",
    "\n",
    "topic_model1n = BERTopic(umap_model=umap_model, \n",
    "                         hdbscan_model=hdbscan_model, \n",
    "                         language=\"multilingual\", \n",
    "                         calculate_probabilities=True, \n",
    "                         nr_topics=\"auto\",\n",
    "                         vectorizer_model=vectorizer_model,\n",
    "                         ctfidf_model=ctfidf_model, \n",
    "                         representation_model=representation_model\n",
    "                         )\n",
    "\n",
    "topics_negative_igua, probabilities_negative_igua = topic_model1n.fit_transform(df_nega_igua['tweet_text_limpo'],embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "88d4bb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>457</td>\n",
       "      <td>-1_chacina parque_chacina_ruralistas_compartil...</td>\n",
       "      <td>0.146993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>273</td>\n",
       "      <td>0_morto dentro_veado encontrado_encontrado mor...</td>\n",
       "      <td>0.087810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>261</td>\n",
       "      <td>1_meio parque_biodiversidade_rasgar meio_reman...</td>\n",
       "      <td>0.083950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>227</td>\n",
       "      <td>2_táxis_iguaçu decisão_proíbe_taxis</td>\n",
       "      <td>0.073014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>148</td>\n",
       "      <td>3_atropelada_jaguatirica_próximo parque_atrope...</td>\n",
       "      <td>0.047604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name  Percentage\n",
       "0     -1    457  -1_chacina parque_chacina_ruralistas_compartil...    0.146993\n",
       "1      0    273  0_morto dentro_veado encontrado_encontrado mor...    0.087810\n",
       "2      1    261  1_meio parque_biodiversidade_rasgar meio_reman...    0.083950\n",
       "3      2    227                2_táxis_iguaçu decisão_proíbe_taxis    0.073014\n",
       "4      3    148  3_atropelada_jaguatirica_próximo parque_atrope...    0.047604"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_igua = topic_model1n.get_topic_info().iloc[0:5,]\n",
    "\n",
    "df_topic_igua['Percentage'] = topic_model1n.get_topic_info().Count/topic_model1n.get_topic_info().Count.sum()\n",
    "\n",
    "df_topic_igua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70551afe",
   "metadata": {},
   "source": [
    "#### PARQUE NACIONAL DA TIJUCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "643e6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nega_tiju = pd.DataFrame(df.loc[(df.sentiment=='negative') & (df.UC_text=='PARQUE NACIONAL DA TIJUCA'),\n",
    "                                   'tweet_text_limpo']).reset_index(drop=True)\n",
    "\n",
    "df_nega_tiju['tweet_text_limpo'] = df_nega_tiju['tweet_text_limpo'].astype('string') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "b68655aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate BERTopic\n",
    "embeddings = sentence_model.encode(df_nega_tiju['tweet_text_limpo'], show_progress_bar=False)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=6, \n",
    "                        metric='euclidean', \n",
    "                        prediction_data=True)\n",
    "\n",
    "topic_model2n = BERTopic(umap_model=umap_model, \n",
    "                         hdbscan_model=hdbscan_model, \n",
    "                         language=\"multilingual\", \n",
    "                         calculate_probabilities=True, \n",
    "                         nr_topics=\"auto\",\n",
    "                         vectorizer_model=vectorizer_model,\n",
    "                         ctfidf_model=ctfidf_model, \n",
    "                         representation_model=representation_model)\n",
    "\n",
    "topics_negative_tiju, probabilities_negative_tiju = topic_model2n.fit_transform(df_nega_tiju['tweet_text_limpo'],embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "66571201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>0_ambiental_proteção ambiental_proteção_área</td>\n",
       "      <td>0.205021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>1_tijuca icmbio_tijuca floresta_icmbio_floresta</td>\n",
       "      <td>0.100418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>2_assaltados parque_assaltados_visitantes assa...</td>\n",
       "      <td>0.087866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>3_46_46 anos_mora 46_idoso mora</td>\n",
       "      <td>0.079498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name  Percentage\n",
       "1      0    196       0_ambiental_proteção ambiental_proteção_área    0.205021\n",
       "2      1     96    1_tijuca icmbio_tijuca floresta_icmbio_floresta    0.100418\n",
       "3      2     84  2_assaltados parque_assaltados_visitantes assa...    0.087866\n",
       "4      3     76                    3_46_46 anos_mora 46_idoso mora    0.079498"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_tiju = topic_model2n.get_topic_info().iloc[1:5,]\n",
    "\n",
    "df_topic_tiju['Percentage'] = topic_model2n.get_topic_info().Count/topic_model2n.get_topic_info().Count.sum()\n",
    "\n",
    "df_topic_tiju"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfeb979",
   "metadata": {},
   "source": [
    "#### PARQUE NACIONAL DOS LENÇÓIS MARANHENSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "6d8dcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nega_len = pd.DataFrame(df.loc[(df.sentiment=='negative') & (df.UC_text=='PARQUE NACIONAL DOS LENÇÓIS MARANHENSES'),\n",
    "                                  'tweet_text_limpo']).reset_index(drop=True)\n",
    "\n",
    "df_nega_len['tweet_text_limpo'] = df_nega_len['tweet_text_limpo'].astype('string') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "176591b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate BERTopic\n",
    "embeddings = sentence_model.encode(df_nega_len['tweet_text_limpo'], show_progress_bar=False)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=3, \n",
    "                        metric='euclidean', \n",
    "                        prediction_data=True)\n",
    "\n",
    "topic_model3n = BERTopic(umap_model=umap_model, \n",
    "                         hdbscan_model=hdbscan_model, \n",
    "                         language=\"multilingual\", \n",
    "                         calculate_probabilities=True, \n",
    "                         nr_topics=\"auto\",\n",
    "                         vectorizer_model=vectorizer_model,\n",
    "                         ctfidf_model=ctfidf_model, \n",
    "                         representation_model=representation_model)\n",
    "\n",
    "topics_negative_len, probabilities_negative_len = topic_model3n.fit_transform(df_nega_len['tweet_text_limpo'],embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "2bf53d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>46</td>\n",
       "      <td>-1_maranhenses_filme_maranhenses alguém_requer...</td>\n",
       "      <td>0.141104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0_quadriciclos parque_quadriciclos_trânsito qu...</td>\n",
       "      <td>0.368098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1_projeto_limites_limites parque_altera</td>\n",
       "      <td>0.254601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>2_estradas_risco parque_põem risco_põem</td>\n",
       "      <td>0.070552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3_nacional lençois_lençois_lençois maranhenses...</td>\n",
       "      <td>0.042945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4_acidente_mortos oito_mortos_quatro mortos</td>\n",
       "      <td>0.036810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name  Percentage\n",
       "0     -1     46  -1_maranhenses_filme_maranhenses alguém_requer...    0.141104\n",
       "1      0    120  0_quadriciclos parque_quadriciclos_trânsito qu...    0.368098\n",
       "2      1     83            1_projeto_limites_limites parque_altera    0.254601\n",
       "3      2     23            2_estradas_risco parque_põem risco_põem    0.070552\n",
       "4      3     14  3_nacional lençois_lençois_lençois maranhenses...    0.042945\n",
       "5      4     12        4_acidente_mortos oito_mortos_quatro mortos    0.036810"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_len = topic_model3n.get_topic_info().iloc[0:6,]\n",
    "\n",
    "df_topic_len['Percentage'] = topic_model3n.get_topic_info().Count/topic_model3n.get_topic_info().Count.sum()\n",
    "\n",
    "df_topic_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a12a64",
   "metadata": {},
   "source": [
    "#### PARQUE NACIONAL DE JERICOACOARA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "7af57b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nega_jeri = pd.DataFrame(df.loc[(df.sentiment=='negative') & (df.UC_text=='PARQUE NACIONAL DE JERICOACOARA'),\n",
    "                                   'tweet_text_limpo']).reset_index(drop=True)\n",
    "\n",
    "df_nega_jeri['tweet_text_limpo'] = df_nega_jeri['tweet_text_limpo'].astype('string') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "ba401ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = sentence_model.encode(df_nega_jeri['tweet_text_limpo'], show_progress_bar=False)\n",
    "\n",
    "#Due to the low number of observations, we reduced the value of the hyperparameter \"min_cluster_sized\"\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=3, \n",
    "                        metric='euclidean', \n",
    "                        prediction_data=True)\n",
    "\n",
    "topic_model4n = BERTopic(umap_model=umap_model, \n",
    "                         hdbscan_model=hdbscan_model, \n",
    "                         language=\"multilingual\", \n",
    "                         calculate_probabilities=True, \n",
    "                         nr_topics=\"auto\",\n",
    "                         vectorizer_model=vectorizer_model,\n",
    "                         ctfidf_model=ctfidf_model, \n",
    "                         representation_model=representation_model)\n",
    "\n",
    "topics_negative_jeri, probabilities_negative_jeri = topic_model4n.fit_transform(df_nega_jeri['tweet_text_limpo'],embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c076f81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>-1_unidades_nordeste_jericoacoara reserva_lista</td>\n",
       "      <td>0.073333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0_pedra furada_pedra_furada_invasão parque</td>\n",
       "      <td>0.186667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1_turismo_favor_pra_sendo</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>2_jericoacoara governo_vai privatizar_anuncia_...</td>\n",
       "      <td>0.113333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3_dia reabertura_pouca movimentação_jericoacoa...</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4_federal aprova_jericoacoara iniciativa_aprov...</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name  Percentage\n",
       "0     -1     11    -1_unidades_nordeste_jericoacoara reserva_lista    0.073333\n",
       "1      0     28         0_pedra furada_pedra_furada_invasão parque    0.186667\n",
       "2      1     21                          1_turismo_favor_pra_sendo    0.140000\n",
       "3      2     17  2_jericoacoara governo_vai privatizar_anuncia_...    0.113333\n",
       "4      3     10  3_dia reabertura_pouca movimentação_jericoacoa...    0.066667\n",
       "5      4     10  4_federal aprova_jericoacoara iniciativa_aprov...    0.066667"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_jeri = topic_model4n.get_topic_info().iloc[0:6,]\n",
    "\n",
    "df_topic_jeri['Percentage'] = topic_model4n.get_topic_info().Count/topic_model4n.get_topic_info().Count.sum()\n",
    "\n",
    "df_topic_jeri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0aec69",
   "metadata": {},
   "source": [
    "#### PARQUE NACIONAL DA SERRA DA BOCAINA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "504c55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nega_boca = pd.DataFrame(df.loc[(df.sentiment=='negative') & (df.UC_text=='PARQUE NACIONAL DA SERRA DA BOCAINA'),\n",
    "                                   'tweet_text_limpo']).reset_index(drop=True)\n",
    "\n",
    "df_nega_boca['tweet_text_limpo'] = df_nega_boca['tweet_text_limpo'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "0877f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = sentence_model.encode(df_nega_boca['tweet_text_limpo'], show_progress_bar=False)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=2, \n",
    "                        metric='euclidean', \n",
    "                        prediction_data=True)\n",
    "\n",
    "topic_model5n = BERTopic(umap_model=umap_model, \n",
    "                         hdbscan_model=hdbscan_model, \n",
    "                         language=\"multilingual\", \n",
    "                         calculate_probabilities=True, \n",
    "                         nr_topics=\"auto\",\n",
    "                         vectorizer_model=vectorizer_model,\n",
    "                         ctfidf_model=ctfidf_model, \n",
    "                         representation_model=representation_model)\n",
    "\n",
    "topics_negative_boca, probabilities_negative_boca = topic_model5n.fit_transform(df_nega_boca['tweet_text_limpo'],embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "375add82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1_dentro parque_dentro_impacto_nome</td>\n",
       "      <td>0.095890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0_hectares_600 hectares_hectares parque_destruiu</td>\n",
       "      <td>0.260274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1_frade_após queimada_bandidos_icmbio</td>\n",
       "      <td>0.136986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2_condenação_mantém condenação_bocaina trf2_co...</td>\n",
       "      <td>0.095890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3_interior parque_interior_caçadores sobrevoar...</td>\n",
       "      <td>0.082192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4_prefeito paraty_acabar_acabar parque_quer ac...</td>\n",
       "      <td>0.068493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name  Percentage\n",
       "0     -1      7               -1_dentro parque_dentro_impacto_nome    0.095890\n",
       "1      0     19   0_hectares_600 hectares_hectares parque_destruiu    0.260274\n",
       "2      1     10              1_frade_após queimada_bandidos_icmbio    0.136986\n",
       "3      2      7  2_condenação_mantém condenação_bocaina trf2_co...    0.095890\n",
       "4      3      6  3_interior parque_interior_caçadores sobrevoar...    0.082192\n",
       "5      4      5  4_prefeito paraty_acabar_acabar parque_quer ac...    0.068493"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_boca = topic_model5n.get_topic_info().iloc[0:6,]\n",
    "\n",
    "df_topic_boca['Percentage'] = topic_model5n.get_topic_info().Count/topic_model5n.get_topic_info().Count.sum()\n",
    "\n",
    "df_topic_boca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4981cc",
   "metadata": {},
   "source": [
    "#### PARQUE NACIONAL MAR. DE FERNANDO DE NORONHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "1cfdc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nega_noro = pd.DataFrame(df.loc[(df.sentiment=='negative') & (df.UC_text=='PARQUE NACIONAL MAR. DE FERNANDO DE NORONHA'),\n",
    "                                   'tweet_text_limpo']).reset_index(drop=True)\n",
    "\n",
    "df_nega_noro['tweet_text_limpo'] = df_nega_noro['tweet_text_limpo'].astype('string') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "786c250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = sentence_model.encode(df_nega_noro['tweet_text_limpo'], show_progress_bar=False)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=3, \n",
    "                        metric='euclidean', \n",
    "                        prediction_data=True)\n",
    "\n",
    "topic_model6n = BERTopic(umap_model=umap_model, \n",
    "                         hdbscan_model=hdbscan_model, \n",
    "                         language=\"multilingual\", \n",
    "                         calculate_probabilities=True, \n",
    "                         nr_topics=\"auto\",\n",
    "                         vectorizer_model=vectorizer_model,\n",
    "                         ctfidf_model=ctfidf_model, \n",
    "                         representation_model=representation_model)\n",
    "\n",
    "topics_negative_noro, probabilities_negative_noro = topic_model6n.fit_transform(df_nega_noro['tweet_text_limpo'],embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "d3653817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "      <td>-1_causa_pandemia_proteção ambiental_proteção</td>\n",
       "      <td>0.147436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0_regras_mudam_atrativos_atrativos parque</td>\n",
       "      <td>0.282051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1_meio_meio ambiente_ambiente_pernambuco</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2_abrir_abrir pesca_hora_dentro parque</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3_garrafas_lixo_70 lixo_garrafas pet</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4_jatinho_salles_sardinha_pesca sardinha</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                           Name  Percentage\n",
       "0     -1     23  -1_causa_pandemia_proteção ambiental_proteção    0.147436\n",
       "1      0     44      0_regras_mudam_atrativos_atrativos parque    0.282051\n",
       "2      1     42       1_meio_meio ambiente_ambiente_pernambuco    0.269231\n",
       "3      2      7         2_abrir_abrir pesca_hora_dentro parque    0.044872\n",
       "4      3      7           3_garrafas_lixo_70 lixo_garrafas pet    0.044872\n",
       "5      4      6       4_jatinho_salles_sardinha_pesca sardinha    0.038462"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_noro = topic_model6n.get_topic_info().iloc[0:6,]\n",
    "\n",
    "df_topic_noro['Percentage'] = topic_model6n.get_topic_info().Count/topic_model6n.get_topic_info().Count.sum()\n",
    "\n",
    "df_topic_noro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca90bf",
   "metadata": {},
   "source": [
    "## Principal topics in negative tweets for all national parks in Brazil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c95c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nega_all = pd.DataFrame(df.loc[(df.sentiment=='negative'),\n",
    "                                  'tweet_text_limpo']).reset_index(drop=True)\n",
    "\n",
    "df_nega_all['tweet_text_limpo'] = df_nega_all['tweet_text_limpo'].astype('string') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f30d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate BERTopic\n",
    "embeddings = sentence_model.encode(df_nega_all['tweet_text_limpo'], show_progress_bar=False)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=20, \n",
    "                        metric='euclidean', \n",
    "                        prediction_data=True)\n",
    "\n",
    "topic_model_all = BERTopic(umap_model=umap_model, \n",
    "                         hdbscan_model=hdbscan_model, \n",
    "                         language=\"multilingual\", \n",
    "                         calculate_probabilities=True, \n",
    "                         nr_topics=\"auto\",\n",
    "                         vectorizer_model=vectorizer_model,\n",
    "                         ctfidf_model=ctfidf_model, \n",
    "                         representation_model=representation_model)\n",
    "\n",
    "topics_negative, probabilities_negative = topic_model_all.fit_transform(df_nega_all['tweet_text_limpo'],embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fab05d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>5060</td>\n",
       "      <td>-1_pode_ão_meio_hoje</td>\n",
       "      <td>0.275179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1038</td>\n",
       "      <td>0_ba meses_pascoal ba_meses icmbio_icmbio incê...</td>\n",
       "      <td>0.056450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>586</td>\n",
       "      <td>1_diamantina incêndio_gigantesco consumindo_in...</td>\n",
       "      <td>0.031869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>381</td>\n",
       "      <td>2_colono_estrada colono_aceitamos_atlântica bi...</td>\n",
       "      <td>0.020720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>377</td>\n",
       "      <td>3_incêndio fecha_cipó minas_gerais incêndio_fe...</td>\n",
       "      <td>0.020503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>375</td>\n",
       "      <td>4_pra_noronha_cara_ministro</td>\n",
       "      <td>0.020394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>369</td>\n",
       "      <td>5_tava_ir_pro_pro parque</td>\n",
       "      <td>0.020067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>355</td>\n",
       "      <td>6_óleo_manchas_manchas óleo_óleo chegam</td>\n",
       "      <td>0.019306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>332</td>\n",
       "      <td>7_proporções atinge_brasília incêndio_incêndio...</td>\n",
       "      <td>0.018055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>324</td>\n",
       "      <td>8_acabou publicar_publicar foto_publicar_foto ...</td>\n",
       "      <td>0.017620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name  Percentage\n",
       "0     -1   5060                               -1_pode_ão_meio_hoje    0.275179\n",
       "1      0   1038  0_ba meses_pascoal ba_meses icmbio_icmbio incê...    0.056450\n",
       "2      1    586  1_diamantina incêndio_gigantesco consumindo_in...    0.031869\n",
       "3      2    381  2_colono_estrada colono_aceitamos_atlântica bi...    0.020720\n",
       "4      3    377  3_incêndio fecha_cipó minas_gerais incêndio_fe...    0.020503\n",
       "5      4    375                        4_pra_noronha_cara_ministro    0.020394\n",
       "6      5    369                           5_tava_ir_pro_pro parque    0.020067\n",
       "7      6    355            6_óleo_manchas_manchas óleo_óleo chegam    0.019306\n",
       "8      7    332  7_proporções atinge_brasília incêndio_incêndio...    0.018055\n",
       "9      8    324  8_acabou publicar_publicar foto_publicar_foto ...    0.017620"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_all = topic_model_all.get_topic_info().iloc[0:10,]\n",
    "\n",
    "df_topic_all['Percentage'] = topic_model_all.get_topic_info().Count/topic_model_all.get_topic_info().Count.sum()\n",
    "\n",
    "df_topic_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e140b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
